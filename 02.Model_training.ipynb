{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7659b8c1",
   "metadata": {},
   "source": [
    "<img src=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Faws.amazon.com%2Fes%2Fsolutions%2Fcase-studies%2FKueski%2F&psig=AOvVaw2G4P4XL7b6yQb4dv5CCv6i&ust=1640206653391000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCOjUsInk9fQCFQAAAAAdAAAAABAD\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f9dd3",
   "metadata": {},
   "source": [
    "## Glue Job Preprocessing with PySpark- Model training-Kueski Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b07dd",
   "metadata": {},
   "source": [
    "AWS Glue supports an extension of the PySpark Python dialect for scripting extract, transform, and load (ETL) jobs. In this notebook we use this dialect for creating an ETL script to run a Glue job. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85385baa",
   "metadata": {},
   "source": [
    "<a id='contents' />\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Loading libraries](#loading)\n",
    "2. [Data Preprocessing. Data Pipelines](#etl)\n",
    "3. [Model Training.](#s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5326cc",
   "metadata": {},
   "source": [
    "<a id='loading' />\n",
    "\n",
    "## 1. Loading libraries:\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71cf56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>165</td><td>application_1622637872219_0165</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-171-232.ec2.internal:20888/proxy/application_1622637872219_0165/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-179-128.ec2.internal:8042/node/containerlogs/container_1622637872219_0165_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the required libraries"
     ]
    }
   ],
   "source": [
    "print('Loading the required libraries')\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "kms_key_id = '36f1041a-656a-4df4-954a-49b6a39e4b54'\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "    (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "    (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id),\n",
    "    (\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"),\n",
    "    (\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"),\n",
    "    (\"spark.sql.catalogImplementation\", \"hive\"),\n",
    "    (\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "])\n",
    "sc = SparkContext.getOrCreate(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ca5661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dejar variables del processing_job en el proceso.\n",
    "# Definir variable updated_at. ( podria ser una particion para quedarme solo con aquellas particiones que son las ultimas)\n",
    "# Primary key deberia ser el loan_id. Indice sobre el id del cliente.\n",
    "today       = datetime.strptime(\"2021-10-21\", '%Y-%m-%d').date()\n",
    "updated_at  = today.strftime('%Y-%m-%d')\n",
    "source_path = \"s3://arcosmtk-working-directory/credits_scoring_test/offline_serving/\"\n",
    "# In order to evaluate the performance (backtesting) to measure model degradation. e.g. KS score, AUC or Recall\n",
    "# Date of the simulated data collection.\n",
    "ObservationDate   = \"2020-12-21\"\n",
    "ObservationPeriod = 12\n",
    "PerformancePeriod = 12\n",
    "#TODO create routines to measure degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e704173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Esto lo deberia leer desde el catalogo. Control de columnas y permisos.GDPR\n",
    "df = glueContext.read.parquet(source_path)\\\n",
    "    .select('id','nb_previous_loans','avg_amount_loans_previous','age','years_on_the_job','flag_own_car','status','loan_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5ec447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Skipped step in the original training notebook. Pseudo-replicas problem. Id must be unique. \n",
    "df = df.orderBy(col(\"id\").asc(),col(\"loan_date\").desc()).dropDuplicates(['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f129dd",
   "metadata": {},
   "source": [
    "<a id='etl' />\n",
    "\n",
    "## Data Preprocessing. Data Pipelines.\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f591f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Value counts\n",
    "vc =  df.groupby('status').count()\n",
    "vc = vc.withColumn('freq', col('count')/float(df.count()))\n",
    "vc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca784898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep just selected features by DS.\n",
    "df = df.select('nb_previous_loans','avg_amount_loans_previous','age','years_on_the_job','flag_own_car','status')\n",
    "# fillna #TODO improved by Pipeline.\n",
    "df = df.na.fill(0)\n",
    "# setting X,y.\n",
    "X = df.select('nb_previous_loans','avg_amount_loans_previous','age','years_on_the_job','flag_own_car')\n",
    "y = df.select('status')\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b38c3",
   "metadata": {},
   "source": [
    "<a id='etl' />\n",
    "\n",
    "## Model Training.\n",
    "[(back to top)](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba674856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7113d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
